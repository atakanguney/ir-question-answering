{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:36:49.711820Z",
     "start_time": "2019-04-23T12:36:48.908281Z"
    }
   },
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T10:05:50.416623Z",
     "start_time": "2019-04-23T09:49:41.640123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"tr_imst\" for language \"tr\".\n",
      "Would you like to download the models for: tr_imst now? (Y/n)\n",
      "y\n",
      "\n",
      "Default download directory: /Users/atakan1/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: tr_imst\n",
      "Download location: /Users/atakan1/stanfordnlp_resources/tr_imst_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.77G/1.77G [14:56<00:00, 3.94MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /Users/atakan1/stanfordnlp_resources/tr_imst_models.zip\n",
      "Extracting models file for: tr_imst\n",
      "Cleaning up...Done.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell only ONCE\n",
    "stanfordnlp.download(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:38:09.635092Z",
     "start_time": "2019-04-23T12:37:10.788681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/atakan1/stanfordnlp_resources/tr_imst_models/tr_imst_tokenizer.pt', 'lang': 'tr', 'shorthand': 'tr_imst', 'mode': 'predict'}\n",
      "---\n",
      "Loading: mwt\n",
      "With settings: \n",
      "{'model_path': '/Users/atakan1/stanfordnlp_resources/tr_imst_models/tr_imst_mwt_expander.pt', 'lang': 'tr', 'shorthand': 'tr_imst', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/atakan1/stanfordnlp_resources/tr_imst_models/tr_imst_tagger.pt', 'pretrain_path': '/Users/atakan1/stanfordnlp_resources/tr_imst_models/tr_imst.pretrain.pt', 'lang': 'tr', 'shorthand': 'tr_imst', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/atakan1/stanfordnlp_resources/tr_imst_models/tr_imst_lemmatizer.pt', 'lang': 'tr', 'shorthand': 'tr_imst', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/atakan1/stanfordnlp_resources/tr_imst_models/tr_imst_parser.pt', 'pretrain_path': '/Users/atakan1/stanfordnlp_resources/tr_imst_models/tr_imst.pretrain.pt', 'lang': 'tr', 'shorthand': 'tr_imst', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(lang=\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:38:47.360376Z",
     "start_time": "2019-04-23T12:38:47.356823Z"
    }
   },
   "outputs": [],
   "source": [
    "PASSAGE = \"../deliverables/derlem.txt\"\n",
    "QUESTION_ANSWER = \"../deliverables/soru_gruplari.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:38:47.956174Z",
     "start_time": "2019-04-23T12:38:47.928917Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(PASSAGE, \"rb\") as f:\n",
    "    passage_text = f.read().decode(\"utf-16\")\n",
    "with open(QUESTION_ANSWER, \"rb\") as f:\n",
    "    question_answer = f.read().decode(\"utf-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:39:20.624956Z",
     "start_time": "2019-04-23T12:39:20.589630Z"
    }
   },
   "outputs": [],
   "source": [
    "passages = passage_text.split(\"\\r\\n\\r\\n\")\n",
    "question_groups = question_answer.split(\"\\r\\n\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:39:22.631074Z",
     "start_time": "2019-04-23T12:39:22.591844Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"^\\d+\")\n",
    "passage_dict = {\n",
    "    int(pattern.match(passage).group()): passage[pattern.match(passage).end() + 1:].strip()\n",
    "    for passage in passages\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:39:23.736670Z",
     "start_time": "2019-04-23T12:39:23.731187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[226, 227, 228]\n",
      "['Madenler, yaşamımızın birçok alanında çeşitli amaçlarla işleyerek kullandığımız doğal kaynaklardır. İnşaat, otomotiv, beyaz eşya, mutfak eşyaları, elektrikli ve elektronik eşya üretimi gibi birçok alanda madenler kullanılmaktadır.', 'Türkiye, maden çeşidi bakımından zengin bir ülkedir. Bu maden yataklarının önemli bir kısmı işlenmektedir. Bir maden yatağı işletmeye açılırken o madenin toplam miktarı yani rezervi belirlenir. Maden içindeki saf metal oranı olan tenör de bir maden yatağının işletilmeye açılıp açılmamasında önem taşıyan özelliklerdendir. Çünkü miktarı fazla olsa da tenörü çok düşük olan yataklar işletmeye açılmamaktadır. Bunların yanı sıra işletme maliyeti hesaplanmakta ve kârlı olabilecek yataklar işletilmektedir.', 'Ülkemizde çıkarılan madenlerden bazıları metaliktir: demir, bakır, krom, kurşun ve çinko gibi. Bor, kükürt, asbest, fosfat gibi bazı madenler ise metalik değildir. Şimdi ülkemizdeki başlıca madenleri inceleyelim.']\n"
     ]
    }
   ],
   "source": [
    "print(list(passage_dict.keys())[:3])\n",
    "print(list(passage_dict.values())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:39:32.128921Z",
     "start_time": "2019-04-23T12:39:32.124582Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_passage = list(passage_dict.values())[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:39:33.953034Z",
     "start_time": "2019-04-23T12:39:32.764549Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(sample_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:39:34.291395Z",
     "start_time": "2019-04-23T12:39:34.280992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Sentence in module stanfordnlp.pipeline.doc object:\n",
      "\n",
      "class Sentence(builtins.object)\n",
      " |  Sentence(tokens)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, tokens)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build_dependencies(self)\n",
      " |  \n",
      " |  print_dependencies(self)\n",
      " |  \n",
      " |  print_tokens(self)\n",
      " |  \n",
      " |  print_words(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  dependencies\n",
      " |      Access list of dependencies for this sentence.\n",
      " |  \n",
      " |  tokens\n",
      " |      Access list of tokens for this sentence.\n",
      " |  \n",
      " |  words\n",
      " |      Access list of words for this sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(doc.sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:39:36.926007Z",
     "start_time": "2019-04-23T12:39:36.918714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fosfat', '17', 'nsubj')\n",
      "('ayrıca', '3', 'advmod')\n",
      "('yem', '1', 'conj')\n",
      "(',', '5', 'punct')\n",
      "('gıda', '1', 'conj')\n",
      "(',', '7', 'punct')\n",
      "('deterjan', '1', 'conj')\n",
      "(',', '9', 'punct')\n",
      "('kâğıt', '1', 'conj')\n",
      "(',', '11', 'punct')\n",
      "('kibrit', '1', 'conj')\n",
      "('ve', '13', 'cc')\n",
      "('kimya', '14', 'nmod:poss')\n",
      "('sanayisinin', '1', 'conj')\n",
      "('de', '14', 'advmod:emph')\n",
      "('ham', '17', 'amod')\n",
      "('maddelerinden', '0', 'root')\n",
      "('dir', '17', 'cop')\n",
      "('.', '17', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[1].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:39:37.598744Z",
     "start_time": "2019-04-23T12:39:37.580780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Fosfatın \tlemma: fosfat\tupos: NOUN\txpos: Noun\n",
      "text: büyük \tlemma: büyük\tupos: ADJ\txpos: Adj\n",
      "text: bir \tlemma: bir\tupos: NUM\txpos: ANum\n",
      "text: kısmı \tlemma: kısım\tupos: NOUN\txpos: Noun\n",
      "text: gübre \tlemma: gübre\tupos: NOUN\txpos: Noun\n",
      "text: üretiminde \tlemma: üre\tupos: NOUN\txpos: Noun\n",
      "text: kullanılmaktadır \tlemma: kullan\tupos: VERB\txpos: Verb\n",
      "text: . \tlemma: .\tupos: PUNCT\txpos: Punc\n",
      "text: Fosfat \tlemma: Fosfat\tupos: NOUN\txpos: Noun\n",
      "text: ayrıca \tlemma: ayrıca\tupos: ADV\txpos: Adverb\n",
      "text: yem \tlemma: ye\tupos: NOUN\txpos: Noun\n",
      "text: , \tlemma: ,\tupos: PUNCT\txpos: Punc\n",
      "text: gıda \tlemma: gıda\tupos: NOUN\txpos: Noun\n",
      "text: , \tlemma: ,\tupos: PUNCT\txpos: Punc\n",
      "text: deterjan \tlemma: deterjan\tupos: NOUN\txpos: Noun\n",
      "text: , \tlemma: ,\tupos: PUNCT\txpos: Punc\n",
      "text: kâğıt \tlemma: kâğıt\tupos: ADJ\txpos: NAdj\n",
      "text: , \tlemma: ,\tupos: PUNCT\txpos: Punc\n",
      "text: kibrit \tlemma: kibrit\tupos: NOUN\txpos: Noun\n",
      "text: ve \tlemma: ve\tupos: CCONJ\txpos: Conj\n",
      "text: kimya \tlemma: kimya\tupos: NOUN\txpos: Noun\n",
      "text: sanayisinin \tlemma: sanayi\tupos: NOUN\txpos: Noun\n",
      "text: de \tlemma: de\tupos: CCONJ\txpos: Conj\n",
      "text: ham \tlemma: ham\tupos: ADJ\txpos: Adj\n",
      "text: maddelerinden \tlemma: madde\tupos: NOUN\txpos: Noun\n",
      "text: dir \tlemma: i\tupos: AUX\txpos: Zero\n",
      "text: . \tlemma: .\tupos: PUNCT\txpos: Punc\n",
      "text: ülkemizde \tlemma: ülke\tupos: NOUN\txpos: Noun\n",
      "text: ki \tlemma: ki\tupos: ADP\txpos: Rel\n",
      "text: başlıca \tlemma: başlıca\tupos: ADJ\txpos: Adj\n",
      "text: fosfat \tlemma: fosfat\tupos: NOUN\txpos: Noun\n",
      "text: yatakları \tlemma: yatak\tupos: NOUN\txpos: Noun\n",
      "text: Bingöl \tlemma: bingöl\tupos: PROPN\txpos: Prop\n",
      "text: , \tlemma: ,\tupos: PUNCT\txpos: Punc\n",
      "text: Bitlis \tlemma: Bitlis\tupos: PROPN\txpos: Prop\n",
      "text: , \tlemma: ,\tupos: PUNCT\txpos: Punc\n",
      "text: Hatay \tlemma: Hatay\tupos: PROPN\txpos: Prop\n",
      "text: , \tlemma: ,\tupos: PUNCT\txpos: Punc\n",
      "text: Gaziantep \tlemma: Gaziantep\tupos: PROPN\txpos: Prop\n",
      "text: , \tlemma: ,\tupos: PUNCT\txpos: Punc\n",
      "text: Şanlıurfa \tlemma: şanlıurfa\tupos: PROPN\txpos: Prop\n",
      "text: ve \tlemma: ve\tupos: CCONJ\txpos: Conj\n",
      "text: Mardin'de \tlemma: Mardin\tupos: PROPN\txpos: Prop\n",
      "text: dir \tlemma: i\tupos: AUX\txpos: Zero\n",
      "text: . \tlemma: .\tupos: PUNCT\txpos: Punc\n",
      "text: Fosfatın \tlemma: fosfat\tupos: NOUN\txpos: Noun\n",
      "text: işlendiği \tlemma: işle\tupos: VERB\txpos: Verb\n",
      "text: yer \tlemma: yer\tupos: NOUN\txpos: Noun\n",
      "text: ise \tlemma: ise\tupos: CCONJ\txpos: Conj\n",
      "text: Mazıdağı \tlemma: mazıdağ\tupos: PROPN\txpos: Prop\n",
      "text: 'dır \tlemma: i\tupos: AUX\txpos: Zero\n",
      "text: ( \tlemma: (\tupos: PUNCT\txpos: Punc\n",
      "text: Mardin \tlemma: Mardin\tupos: PROPN\txpos: Prop\n",
      "text: ) \tlemma: )\tupos: PUNCT\txpos: Punc\n",
      "text: . \tlemma: .\tupos: PUNCT\txpos: Punc\n"
     ]
    }
   ],
   "source": [
    "print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
